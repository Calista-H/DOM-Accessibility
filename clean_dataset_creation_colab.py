# -*- coding: utf-8 -*-
"""CLEA Dataset Creation Colab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kKpwybEBY9MVINU4KIPNUXm_GEB65OAM
"""

import numpy as np
import pandas as pd
import os
import glob
import shutil
import requests
from bs4 import BeautifulSoup

# use to remove all .json files starting with 'data'
def remove_files_starting_with(pattern):
    files_to_remove = glob.glob(pattern)
    for file_path in files_to_remove:
        try:
            if os.path.isfile(file_path):  # check if it's a file
                os.remove(file_path)
                #print(f"File '{file_path}' removed successfully.")
        except OSError as e:
            print(f"Error while removing file '{file_path}': {e}")

# use to get DOM representation of webpage
def get_dom(url):
    try:
        response = requests.get(url)
        if response.status_code == 200: # means successful request
            html_content = response.text
            soup = BeautifulSoup(html_content, 'html.parser') # create soup object representing DOM of webpage
            return soup
        else:
            print(f"Failed to fetch the page. Status code: {response.status_code}")
            return None

    except requests.exceptions.RequestException as e:
        print(f"An error occurred: {e}")
        return None

# add DOM representations to DOM column of df - calls get_dom function
def update_dom(df):

  temp_dom = []
  for i in range(len(df.index)):
    temp_dom.append(get_dom(df['webURL'][i]))

  temp_ser = pd.Series(data = temp_dom)
  df["DOM"] = temp_ser

# turn url into dataframe of violations - no rows for no violations
def url2violations (url):
    with open("./tests/example.spec.ts", "w") as f:
      f.write(f"""


      // @ts-check
      const {{ test, expect }} = require('@playwright/test');
      const AxeBuilder = require('@axe-core/playwright').default;
      const fileReader = require('fs');

      test('all violations', async ({{ page }}) => {{
        await page.goto("{url}");

        const accessibilityScanResults = await new AxeBuilder({{ page }}).analyze(); // 4

        const violations = accessibilityScanResults.violations

        fileReader.writeFile("num_violations.txt", String(violations.length), function(err) {{
          if (err) console.log(err);
        }})

        // read violations individually into separate .json files
        for (let i = 0; i < violations.length; i++) {{
          fileReader.writeFile("data" + i + ".json", JSON.stringify(violations[i]), function(err) {{
            if (err) console.log(err);
          }})
        }}
      }});


      """)

    os.system("CI=1 npx playwright test")

    # store the num_violations in a length variable
    length_file = open('num_violations.txt', "r")
    length = int(length_file.readline())

    df = pd.DataFrame()

    # build dataset by concatenating individual rows violations
    if length > 0:
      for i in range(length):
        df_temp = pd.read_json("data" + str(i) + ".json", lines=True)
        df_temp = df_temp.reset_index(drop = True)
        df = pd.concat([df, df_temp])
      df.insert(0, "webURL", url)
      df.insert(1, "numViolations", length)

      # extract html and failure summary from nodes column
      df['html'] = [[[node_item['html'] for node_item in nodes]] for nodes in df['nodes']]
      df['failureSummary'] = [[[node_item['failureSummary'] for node_item in nodes]] for nodes in df['nodes']]
      #drop the nodes column
      df.drop(['nodes'], axis = 1, inplace = True)

      # add row index
      df = df.reset_index(drop=True)

      #directory = remove the https://
      dom_dir = url[url.index('//') + 2:]
      dom_file_name = dom_dir + 'index.html'

      # add the dom column
      update_dom(df)

      # delete data.json's to reset for the next round
      remove_files_starting_with("data*")

      # remove num_violations.txt file
      if os.path.exists('num_violations.txt'):
        os.remove('num_violations.txt')

      # remove dom file and directory
      if os.path.exists(dom_file_name):
        os.remove(dom_file_name)
      if os.path.exists(dom_dir):
        shutil.rmtree(dom_dir)

    return df

df_urls = pd.read_csv('/Users/suchir/Downloads/100httpURLs.csv')

# run dataframe creation on URL csv
df = pd.DataFrame()
for index, row in df_urls.iterrows():
  #print current URL for progress
  print(row['URL'])

  #concatenate individual rows
  df_temp = url2violations(row['URL'])
  df = pd.concat([df, df_temp])


# count number of unique web URLs
df['webURL'].nunique()

# list number of violations for each URL
df.value_counts(df['webURL'][80:])

# save created dataset
df.to_csv('urlViolations.csv')